# -*- coding: utf-8 -*-
"""Kalbe_Regression__Clustering_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xs1sGxr3xy5KO6QrabD1mAIambXed2cL

# PERSONAL DATA

### **Nurkholiq Agani Hafid**

Instagram: [mecoliqnur](https://www.instagram.com/mecoliqnur/)

Portfolio:
* [LinkedIn](https://www.linkedin.com/in/nurkholiqaganihafid/)
* [GitHub](https://github.com/nurkholiqaganihafid)

# DESCRIPTION

Mengembangkan **model prediktif** untuk meningkatkan bisnis perusahaan, termasuk mengoptimalkan strategi bisnis dan melakukan analisis regresi dan clustering.
- Model Machine Learning Regression (Time Series): Memprediksi total quantity harian penjualan produk untuk merencanakan persediaan dan meningkatkan efisiensi bisnis.
- Model Machine Learning Clustering: Membuat cluster pelanggan dengan kesamaan tertentu untuk memahami pelanggan lebih baik dan merancang strategi pemasaran yang lebih efektif.

The data link contains each column's information: [Case Study - Legend](https://github.com/nurkholiqaganihafid/Kalbe_Nutritionals_Data_Scientist_Project/blob/main/content/Case%20Study%20-%20Legend.txt)

# DATA PREPARATION

## Libraries
"""

!pip install pmdarima

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
import math

from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split
from statsmodels.tsa.stattools import adfuller
from pmdarima import auto_arima
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

import warnings
warnings.filterwarnings('ignore')

plt.rc('axes', grid=True)

# %matplotlib inline

"""## Load Dataset

### Customer
"""

customer = pd.read_csv('/content/Customer.csv', sep=';')
customer.head()

"""### Product"""

product = pd.read_csv('/content/Product.csv', sep=';')
product.head()

"""### Store"""

store = pd.read_csv('/content/Store.csv', sep=';')
store.head()

"""### Transaction"""

transaction = pd.read_csv('/content/Transaction.csv', sep=';')
transaction.head()

"""## Cleaning Data

### Customer
"""

customer.info()

customer.CustomerID.nunique()

total_duplicates = customer.duplicated().sum()
print('Total duplicates of customer:', total_duplicates)

"""Merubah tipe data Income menjadi Float dan koma menjadi titik pada nilai pada kolom Income"""

customer['Income'] = customer['Income'].str.replace(',', '.').astype(float)

customer['Income']

"""### Product"""

product.info()

product.ProductID.nunique()

total_duplicates = product.duplicated().sum()
print('Total duplicates of product:', total_duplicates)

"""### Store"""

store.info()

store.StoreID.nunique()

total_duplicates = store.duplicated().sum()
print('Total duplicates of store:', total_duplicates)

"""Merubah tipe data Latitude dan Longitude menjadi Float dan nilai koma menjadi titik"""

store['Latitude'] = store['Latitude'].str.replace(',', '.').astype(float)
store['Longitude'] = store['Longitude'].str.replace(',', '.').astype(float)

store['Latitude'].head()

store['Longitude'].head()

"""### Transaction"""

transaction.info()

nunique_counts = transaction['TransactionID'].nunique()
nunique_counts

"""- Outup pada nunique() pada kolom TransactionID tidak sama dengan index aslinya bahwa kolom tersebut terjadi duplikat"""

# Memfilter data yang memiliki jumlah nilai unik yang sama dengan 'nunique_counts'
result = transaction.groupby('TransactionID').filter(lambda x: x['TransactionID'].nunique() == nunique_counts)
print(result)

# Mencetak semua baris yang memiliki duplikat di kolom TransactionID
duplicate_rows = transaction[transaction.duplicated(subset=['TransactionID'], keep=False)]
print(duplicate_rows)

"""The last step, memastikan apakah data pada setiap kolom ada duplikat"""

total_duplicates = transaction.duplicated().sum()
print('Total duplicates of transaction:', total_duplicates)

"""- Setiap baris pada kolom Transaction tidak ada data duplikat"""

transaction.head(1)

"""Merubah tipe data Date menjadi datetime"""

transaction['Date'] = pd.to_datetime(transaction['Date'], format='%d/%m/%Y')
transaction.head()

transaction.dtypes

"""## Handling Missing Values

### Customer
"""

customer.isnull().sum()

customer.dropna(subset=['Marital Status'], inplace=True)

customer.isnull().sum()

"""### Product"""

product.isnull().sum()

"""### Store"""

product.isnull().sum()

"""### Transaction"""

transaction.isnull().sum()

"""## Merging All Data Frames"""

df = pd.merge(transaction, customer, on='CustomerID')
df.head(1)

df = pd.merge(df, product.drop(columns='Price'), on='ProductID')
df.head(1)

df = pd.merge(df, store, on='StoreID')
df.sample(2)

df.rename(columns={'TransactionID': 'transaction_id', 'CustomerID': 'customer_id',
                   'ProductID': 'product_id', 'TotalAmount': 'total_amount',
                   'StoreID': 'store_id', 'StoreName': 'store_name',
                   'GroupStore': 'group_store'}, inplace=True)
df.rename(columns=str.lower, inplace=True)
df.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)

df.sample(1)

df.info()

df.describe()

"""# TIME SERIES REGRESSION

## Forecasting Data
"""

df_forecast = df.groupby('date')[['qty']].sum()
df_forecast

df_forecast.plot(figsize=(16,6))
plt.tight_layout()

decompose = seasonal_decompose(df_forecast)

plt.figure(figsize=(16, 12))

plt.subplot(311)
plt.plot(decompose.trend)
plt.title('Trend Analysis')
plt.tight_layout()

plt.subplot(312)
plt.plot(decompose.seasonal)
plt.title('Seasonal Pattern')

plt.subplot(313)
plt.plot(decompose.resid)
plt.title('Residual Analysis')
plt.tight_layout()

"""## Checking Stationarity"""

def check_stationarity(timeseries):
    # Dickey-Fuller test:
    df_test = adfuller(timeseries, autolag='AIC')
    print(df_test)
    print()
    df_output = pd.Series(df_test[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in df_test[4].items():
      df_output['Critical Value (%s)'%key] = value
    print(df_output)

check_stationarity(df_forecast['qty'])

def plot_stationarity(timeseries):
    # Rolling statistics
    rolling_mean = timeseries.rolling(window=12).mean()
    rolling_std = timeseries.rolling(window=12).std()

    plt.figure(figsize=(16, 6))
    plt.plot(timeseries, color='blue', label='Original Quantity')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std Dev')
    plt.title('Rolling Statistics', fontsize=16, pad=10)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Dickey-Fuller test:
    result = adfuller(timeseries, autolag='AIC')
    print('Dickey-Fuller Test:')
    print(f'Test Statistic: {result[0]}')
    print(f'P-value: {result[1]}')
    print(f'Number of Lags Used: {result[2]}')
    print(f'Number of Observations Used: {result[3]}')
    print(f'Critical Value (1%): {result[4]["1%"]}')
    print(f'Critical Value (5%): {result[4]["5%"]}')
    print(f'Critical Value (10%): {result[4]["10%"]}')
    print(f'Is Stationary: {result[1] < 0.05}')
    print('----------------------------------')

    if result[1] < 0.05:
      print('Result: Time Series is stationary')
    else:
      print('Result: Time Series is non-stationary')

plot_stationarity(df_forecast['qty'])

"""Dari pengujian Dickey-Fuller diatas bahwa Test Statistic < Critical Value dan P-value < 0.05 maka time series tersebut **stationary**

## ARIMA MODELING

### Train Test Split
"""

X = df_forecast.index
y = df_forecast['qty']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    shuffle=False)

X_train

print(f'X_test data type: {type(X_test)}, Length: {len(X_test)}')

y_train

print(f'y_test data type: {type(y_test)}, Length: {len(y_test)}')

"""### Compiling the model"""

auto_arima_model = auto_arima(y_train, seasonal=False, stepwise=False,
                              suppress_warnings=True, trace=True)
print(auto_arima_model.summary())

"""- Model ARIMA yang terbaik untuk data ini adalah **ARIMA(2,0,2)(0,0,0)[0]** dengan nilai AIC sebesar **2508.280**

#### Modelling ARIMA
"""

model = ARIMA(y_train, order=(2, 0, 2))
model_fit = model.fit()

forecast_steps = len(X_test)
predictions = model_fit.forecast(steps=forecast_steps)

print(predictions)

"""#### Model Evaluation"""

rmse_val = mean_squared_error(y_test, predictions, squared=False)
mae_val = mean_absolute_error(y_test, predictions)
r2_score_val = r2_score(y_test, predictions)
mse_val = mean_squared_error(y_test, predictions)

print('Root Mean Squared Error (RMSE):', round(rmse_val, 3))
print('Mean Absolute Error (MAE):', round(mae_val, 3))
print("R2 Score:", round(r2_score_val, 3))
print("Mean Squared Error (MSE):", round(mse_val, 3))

if mse_val < rmse_val:
  print("Model performance: MSE is preferred")
else:
  print("Model performance: RMSE is preferred")

"""## Data Visualization"""

# Plot Original Data and Prediction
plt.figure(figsize=(8, 4))
plt.plot(y_test.index, y_test, label='Original Data')
plt.plot(y_test.index, predictions, color='black', label='Prediction')
plt.ylabel('Value')
plt.title('Model Prediction')
plt.legend()
plt.tight_layout()

# Plot ARIMA Prediction
plt.figure(figsize=(8, 4))
plt.plot(y_test.index, predictions, color='black', label='ARIMA Prediction')
plt.xlabel('Date')
plt.ylabel('Value')
plt.title('Forecast Quantity Sold')
plt.legend()
plt.tight_layout()

# Plot the prediction results
plt.figure(figsize=(16, 6))
plt.plot(y_train, label='Train Data')
plt.plot(y_test, color='red', label='Test Data')
plt.plot(predictions, color='black', label='ARIMA Prediction')
plt.title('Forecast Quantity Sold', fontsize=16, pad=10);
plt.legend()
plt.tight_layout()

predictions.describe()

"""- Maka hasil prediksi bahwa rata-rata jumlah terjual per hari adalah 50.6 atau sekitar 51"""

df_predictions = pd.DataFrame()
unique_product = df['product_name'].unique()

rows = math.ceil(len(unique_product) / 2)
cols = 2

fig, axs = plt.subplots(nrows=rows, ncols=cols, figsize=(12, rows * 3))

for i, product in enumerate(unique_product):
    row = i // cols
    col = i % cols

    df_new = df[df['product_name'] == product]

    df_new['date'] = pd.to_datetime(df_new['date'])
    df_new.set_index('date', inplace=True)
    df_new = df_new.resample('D').sum().fillna(df_new.mean())

    model = ARIMA(df_new['qty'], order=(2, 0, 2))
    model_fit = model.fit()

    forecast = model_fit.get_forecast(steps=len(X_test))
    forecast_mean = forecast.predicted_mean

    # create a date range for the forecasted period
    forecast_dates = pd.date_range(start=df_new.index[-1] + pd.DateOffset(1), periods=len(X_test), freq='D')

    # add forecasted values to dataframe
    df_predictions[product] = forecast_mean
    df_predictions.index = forecast_dates

    axs[row, col].plot(df_predictions.index, df_predictions[product], label='Forecasted')
    axs[row, col].set_title(product)
    axs[row, col].legend()

plt.tight_layout()

df_predictions

plt.figure(figsize=(14, 8))
plt.plot(df_predictions, lw=2)
plt.legend(df_predictions.columns, bbox_to_anchor=(1, 1))
plt.ylabel('Quantity Sold', fontsize=14, labelpad=10)
plt.xlabel('Date', fontsize=14, labelpad=10)
plt.title('Forecast of Product Quantity Sold in the Next 2 Months', fontsize=16, pad=10)
plt.tight_layout()

df_predictions.describe()

df_mean_value_products = df_predictions.mean().round().astype(int).sort_values(ascending=False)
df_mean_value_products

print('Number of Products per Day:', df_mean_value_products.sum())

df_mean_value_products_sorted = df_mean_value_products.sort_values(ascending=False)
palette = sns.color_palette("viridis", len(df_mean_value_products_sorted))

sns.set_style('darkgrid')
plt.figure(figsize=(10, 6))

for i, value in enumerate(df_mean_value_products_sorted):
    plt.barh(df_mean_value_products_sorted.index[i], value,
             color=palette[i], label=df_mean_value_products_sorted.index[i])

plt.xlabel('Unit')
plt.ylabel('Products')
plt.title('Average Daily Sales Amount for Each Product in the Next 2 Months',
          pad=10);

"""## Conclusion and Recommendation

**Conclusion**
- Based on the time series prediction results, the average number of products sold per day is 51.
- The product with the highest sales is Thai Tea, with an average quantity of 8 per day, followed by Choco Bar and Ginger Candy, with an average quantity of 7 per day.
- The product with the lowest sales is cashew, with an average of 2 per day.

**Recommendation**
- Increasing the daily stock of Thai Tea, Choco Bar and Ginger Candy products to meet high customer demand.

- Additionally, the lowest-selling products, such as Cashew, can be evaluated to determine whether there are opportunities to increase sales with marketing strategies or changes in product packaging or presentation.

# CLUSTERING
"""

df.head()

df.info()

df.describe()

df_cluster = df.groupby(['customer_id']).agg({ 'transaction_id': 'count',
                                              'qty': 'sum',
                                               'total_amount': 'sum'
                                               }).reset_index()

df_cluster

data_cluster = df_cluster.drop(columns=['customer_id'])
data_cluster.head(1)

data_cluster.info()

data_cluster.isnull().sum()

"""## Standardization"""

s_scaler = StandardScaler()
df_cluster_scaled = s_scaler.fit_transform(data_cluster)

df_cluster_scaled

df_culster_scaled = pd.DataFrame(df_cluster_scaled, columns=data_cluster.columns)

df_culster_scaled

"""## Elbow Method"""

clusters = []
k_range = range(1, 11)

# Finding the optimal number of clusters using the elbow method
for k in k_range:
  model = KMeans(n_clusters=k, n_init=10, random_state=42)
  model.fit(df_cluster_scaled)
  clusters.append(model.inertia_)

print([int(round(cluster)) for cluster in clusters])

plt.figure(figsize=(8, 6))
plt.plot(k_range, clusters, marker='o', markerfacecolor= 'm')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.xticks(list(k_range));

"""- Pada plot diatas bahwa cluster terbaik adalah 3"""

from yellowbrick.cluster import KElbowVisualizer

# Determine the best number of clusters
model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model, k=(1, 11))

visualizer.fit(df_cluster_scaled)
plt.xticks(list(k_range))
visualizer.show();

"""## K-Means Model"""

# The best cluster is 3
model_km3 = KMeans(n_clusters=3, n_init='auto', random_state=42)
model_km3.fit(df_cluster_scaled)
df_cluster['cluster'] = model_km3.labels_

df_cluster

df_cluster.describe()

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_cluster, x='qty', y='total_amount', hue='cluster',
                palette=sns.color_palette('hls', 3))

plt.title('Customer Clustering based on Quantity and Total Amount', fontsize=14, pad=10)
plt.xlabel('Quantity', fontsize=12, labelpad=10)
plt.ylabel('Total Amount', fontsize=12, labelpad=10)
plt.legend(title='Clusters', bbox_to_anchor=(1, 1), frameon=True);

df_cluster[df_cluster['cluster'] == 2]

cluster_summary = df_cluster.groupby(['cluster']).agg({
    'customer_id': 'count',
    'transaction_id': 'mean',
    'qty': 'mean',
    'total_amount': 'mean'
}).round()

cluster_summary.rename(columns={
    'customer_id': 'Total Customers',
    'transaction_id': 'Average Transaction ID',
    'qty': 'Average Quantity',
    'total_amount': 'Average Total Amount'
}, inplace=True)

cluster_summary

plt.rc('axes', grid=False)

x_arange = np.arange(len(cluster_summary))
width_bar = 0.25

fig, ax = plt.subplots(figsize=(12, 8))

# Plotting for Total Customers, Average Transaction ID and Average Quantity
for i, metric in enumerate(cluster_summary.columns[0:3]):
    ax.bar(
        x_arange + i*width_bar,
        cluster_summary[metric],
        width_bar,
        label=f'{metric}'
    )
    for j, val in enumerate(cluster_summary[metric]):
        ax.text(j + i*width_bar, val + 2, int(val), ha='center', va='bottom', fontsize=10)

# Plotting Average Total Amount
x_arange_amount = [0.25, 1.25, 2.25]
ax2 = ax.twinx()
ax2.plot(x_arange_amount, cluster_summary['Average Total Amount'],
         color='purple', marker='o', label=cluster_summary.columns[3])
for i, val in enumerate(cluster_summary['Average Total Amount']):
    ax2.text(i + 0.25, val + 6_000, int(val), ha='center', va='bottom', fontsize=10)

ax.set_ylabel('Counts', labelpad=10, fontsize=14)
ax2.set_ylabel('Total Amount (Avg)', labelpad=10, fontsize=14)
plt.title('Summary of Clusters', pad=10, fontsize=16)

# Equalized the position of the x_arange axis and added labels.
ax.set_xticks(x_arange + width_bar)
ax.set_xticklabels(f'Cluster {label}' for label in cluster_summary.index)

lines, labels = ax.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc=0)

# The y-axis value on the second axis
ax2.set_ylim([0, cluster_summary['Average Total Amount'].max() + 20_000])
ax2.set_yticklabels([f'{int(x)}' if x < 1000 else f'{int(x/1000)}K' for x in ax2.get_yticks()])

plt.tight_layout()

"""## Conclusion and Recommendation

**Conclusion**
- Based on the clustering prediction results, there are 3 customer segments.

- Segment 0 has 135 customers with an average of 8 transactions per customer, 27 average items per transaction, and an average total purchase of Rp 229.389.

- Segment 1 has 202 customers with an average of 11 transactions per customer, 41 average items per transaction, and an average total purchase of Rp 363.267.

- Segment 2 has 107 customers with an average of 15 transactions per customer, 58 average items per transaction, and an average total purchase of Rp 525.432.

**Recommendation**
- Segment 0: Customers in this segment have a low transaction frequency, a moderate number of items per transaction, and relatively low total purchases. The recommendation for this segment is to provide special promotions or discounts to encourage customers to make more transactions.

- Segment 1: Customers in this segment have a moderate frequency of transactions, a high number of items per transaction, and a moderate total purchase. The recommendation for this segment is to provide promotions that focus on increasing the number of items per transaction, such as product bundling or special offers for purchases of a certain amount.

- Segment 2: Customers in this segment have high transaction frequency, a high number of items per transaction, and high total purchases. The recommendation for this segment is to provide promotions that focus on customer rewards and recognition, such as loyalty programs or special discounts for loyal customers.
"""